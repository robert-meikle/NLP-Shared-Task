{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for most of the notebook\n",
    "import torch\n",
    "import pandas\n",
    "from transformers import BertModel\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Dict, List\n",
    "from util import load_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_arg_path = \"../data/arguments-training.tsv\"\n",
    "train_label_path = \"../data/labels-training.tsv\"\n",
    "validation_arg_path = \"../data/arguments-validation.tsv\"\n",
    "validation_label_path = \"../data/labels-validation.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Argument ID\tConclusion\tStance\tPremise\n",
      "\n",
      "  Argument ID                                         Conclusion       Stance  \\\n",
      "0      A01001                     Entrapment should be legalized  in favor of   \n",
      "1      A01002                        We should ban human cloning  in favor of   \n",
      "2      A01003                         We should abandon marriage      against   \n",
      "3      A01004                          We should ban naturopathy      against   \n",
      "4      A01005                            We should ban fast food  in favor of   \n",
      "5      A01006        We should end the use of economic sanctions      against   \n",
      "6      A01007               We should abolish capital punishment      against   \n",
      "7      A01008                      We should ban factory farming      against   \n",
      "8      A01009  We should fight for the abolition of nuclear w...      against   \n",
      "9      A01010                   We should prohibit school prayer      against   \n",
      "\n",
      "                                             Premise  \n",
      "0  if entrapment can serve to more easily capture...  \n",
      "1  we should ban human cloning as it will only ca...  \n",
      "2  marriage is the ultimate commitment to someone...  \n",
      "3        it provides a useful income for some people  \n",
      "4  fast food should be banned because it is reall...  \n",
      "5  sometimes economic sanctions are the only thin...  \n",
      "6  capital punishment is sometimes the only optio...  \n",
      "7  factory farming allows for the production of c...  \n",
      "8  nuclear weapons help keep the peace in uncerta...  \n",
      "9  it should be allowed if the student wants to p...  \n",
      "  Argument ID  Self-direction: thought  Self-direction: action  Stimulation  \\\n",
      "0      A01001                        0                       0            0   \n",
      "1      A01002                        0                       0            0   \n",
      "2      A01003                        0                       1            0   \n",
      "3      A01004                        0                       0            0   \n",
      "4      A01005                        0                       0            0   \n",
      "5      A01006                        0                       0            0   \n",
      "6      A01007                        0                       0            0   \n",
      "7      A01008                        0                       0            0   \n",
      "8      A01009                        0                       0            0   \n",
      "9      A01010                        1                       1            0   \n",
      "\n",
      "   Hedonism  Achievement  Power: dominance  Power: resources  Face  \\\n",
      "0         0            0                 0                 0     0   \n",
      "1         0            0                 0                 0     0   \n",
      "2         0            0                 0                 0     0   \n",
      "3         0            0                 0                 0     0   \n",
      "4         0            0                 0                 0     0   \n",
      "5         0            0                 1                 0     0   \n",
      "6         0            0                 0                 0     0   \n",
      "7         0            0                 0                 0     0   \n",
      "8         0            0                 0                 0     0   \n",
      "9         0            0                 0                 0     0   \n",
      "\n",
      "   Security: personal  ...  Tradition  Conformity: rules  \\\n",
      "0                   0  ...          0                  0   \n",
      "1                   0  ...          0                  0   \n",
      "2                   0  ...          0                  0   \n",
      "3                   1  ...          0                  0   \n",
      "4                   1  ...          0                  0   \n",
      "5                   0  ...          0                  0   \n",
      "6                   0  ...          0                  1   \n",
      "7                   1  ...          0                  0   \n",
      "8                   0  ...          0                  0   \n",
      "9                   0  ...          1                  0   \n",
      "\n",
      "   Conformity: interpersonal  Humility  Benevolence: caring  \\\n",
      "0                          0         0                    0   \n",
      "1                          0         0                    0   \n",
      "2                          0         0                    0   \n",
      "3                          0         0                    0   \n",
      "4                          0         0                    0   \n",
      "5                          0         0                    0   \n",
      "6                          0         0                    0   \n",
      "7                          0         0                    1   \n",
      "8                          0         0                    0   \n",
      "9                          0         0                    0   \n",
      "\n",
      "   Benevolence: dependability  Universalism: concern  Universalism: nature  \\\n",
      "0                           0                      0                     0   \n",
      "1                           0                      0                     0   \n",
      "2                           0                      0                     0   \n",
      "3                           0                      0                     0   \n",
      "4                           0                      0                     0   \n",
      "5                           0                      0                     0   \n",
      "6                           0                      1                     0   \n",
      "7                           0                      1                     0   \n",
      "8                           0                      1                     0   \n",
      "9                           0                      1                     0   \n",
      "\n",
      "   Universalism: tolerance  Universalism: objectivity  \n",
      "0                        0                          0  \n",
      "1                        0                          0  \n",
      "2                        0                          0  \n",
      "3                        0                          0  \n",
      "4                        0                          0  \n",
      "5                        0                          0  \n",
      "6                        0                          0  \n",
      "7                        0                          0  \n",
      "8                        0                          0  \n",
      "9                        0                          0  \n",
      "\n",
      "[10 rows x 21 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Argument ID</th>\n",
       "      <th>Conclusion</th>\n",
       "      <th>Stance</th>\n",
       "      <th>Premise</th>\n",
       "      <th>Self-direction: action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A01001</td>\n",
       "      <td>Entrapment should be legalized</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>if entrapment can serve to more easily capture...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A01002</td>\n",
       "      <td>We should ban human cloning</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>we should ban human cloning as it will only ca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A01003</td>\n",
       "      <td>We should abandon marriage</td>\n",
       "      <td>against</td>\n",
       "      <td>marriage is the ultimate commitment to someone...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A01004</td>\n",
       "      <td>We should ban naturopathy</td>\n",
       "      <td>against</td>\n",
       "      <td>it provides a useful income for some people</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A01005</td>\n",
       "      <td>We should ban fast food</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>fast food should be banned because it is reall...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5215</th>\n",
       "      <td>D27096</td>\n",
       "      <td>Nepotism exists in Bollywood</td>\n",
       "      <td>against</td>\n",
       "      <td>Star kids also have an upbringing which is sur...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5216</th>\n",
       "      <td>D27097</td>\n",
       "      <td>Nepotism exists in Bollywood</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>Movie stars of Bollywood often launch their ch...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5217</th>\n",
       "      <td>D27098</td>\n",
       "      <td>India is safe for women</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>Evil historic practices on women in the pre an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5218</th>\n",
       "      <td>D27099</td>\n",
       "      <td>India is safe for women</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>Women of our country have been and are achievi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5219</th>\n",
       "      <td>D27100</td>\n",
       "      <td>India is safe for women</td>\n",
       "      <td>against</td>\n",
       "      <td>The National Crime Records Bureau states that ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5220 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Argument ID                      Conclusion       Stance  \\\n",
       "0         A01001  Entrapment should be legalized  in favor of   \n",
       "1         A01002     We should ban human cloning  in favor of   \n",
       "2         A01003      We should abandon marriage      against   \n",
       "3         A01004       We should ban naturopathy      against   \n",
       "4         A01005         We should ban fast food  in favor of   \n",
       "...          ...                             ...          ...   \n",
       "5215      D27096    Nepotism exists in Bollywood      against   \n",
       "5216      D27097    Nepotism exists in Bollywood  in favor of   \n",
       "5217      D27098         India is safe for women  in favor of   \n",
       "5218      D27099         India is safe for women  in favor of   \n",
       "5219      D27100         India is safe for women      against   \n",
       "\n",
       "                                                Premise  \\\n",
       "0     if entrapment can serve to more easily capture...   \n",
       "1     we should ban human cloning as it will only ca...   \n",
       "2     marriage is the ultimate commitment to someone...   \n",
       "3           it provides a useful income for some people   \n",
       "4     fast food should be banned because it is reall...   \n",
       "...                                                 ...   \n",
       "5215  Star kids also have an upbringing which is sur...   \n",
       "5216  Movie stars of Bollywood often launch their ch...   \n",
       "5217  Evil historic practices on women in the pre an...   \n",
       "5218  Women of our country have been and are achievi...   \n",
       "5219  The National Crime Records Bureau states that ...   \n",
       "\n",
       "      Self-direction: action  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          1  \n",
       "3                          0  \n",
       "4                          0  \n",
       "...                      ...  \n",
       "5215                       0  \n",
       "5216                       0  \n",
       "5217                       0  \n",
       "5218                       0  \n",
       "5219                       0  \n",
       "\n",
       "[5220 rows x 5 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_arguments, df_train_labels = load_training_data(train_arg_path, train_label_path)\n",
    "\n",
    "df_train_labels = df_train_labels[[\"Argument ID\", \"Self-direction: action\"]]\n",
    "df_train_arguments = df_train_arguments.merge(df_train_labels, on='Argument ID')\n",
    "df_train_arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Argument ID\tConclusion\tStance\tPremise\n",
      "\n",
      "unrecognized stance for line 'D03045\tCattle slaughter should be banned\tin favour of\tThe cow is a sacred animal for Hindus. Killing cows hurts their sentiments.\n",
      "', skipping.\n",
      "unrecognized stance for line 'D03051\tHindi should be the national language of India\tin favour of\tEnglish is not native to India, but Hindi is.\n",
      "', skipping.\n",
      "unrecognized stance for line 'D03052\tHindi should be the national language of India\tin favour of\tHindi is spoken by more than half of India. As no single language is spoken by the entire of India, it is much better to make the majoritarian language the national language. \n",
      "', skipping.\n",
      "  Argument ID                                       Conclusion       Stance  \\\n",
      "0      A01001                   Entrapment should be legalized  in favor of   \n",
      "1      A01012  The use of public defenders should be mandatory  in favor of   \n",
      "2      A02001                    Payday loans should be banned  in favor of   \n",
      "3      A02002                       Surrogacy should be banned      against   \n",
      "4      A02009                   Entrapment should be legalized      against   \n",
      "5      A02018     Intelligence tests bring more harm than good      against   \n",
      "6      A03005          Social media brings more harm than good  in favor of   \n",
      "7      A03014                   Entrapment should be legalized  in favor of   \n",
      "8      A03018                       Surrogacy should be banned  in favor of   \n",
      "9      A04005     Intelligence tests bring more harm than good  in favor of   \n",
      "\n",
      "                                             Premise  \n",
      "0  if entrapment can serve to more easily capture...  \n",
      "1  the use of public defenders should be mandator...  \n",
      "2  payday loans create a more impoverished societ...  \n",
      "3  Surrogacy should not be banned as it is the wo...  \n",
      "4                  entrapment is gravely immoral and  \n",
      "5  intelligence tests should continue to be used ...  \n",
      "6  it gives people a platform to pretend, show of...  \n",
      "7    anyone who commits a crime should be prosecuted  \n",
      "8  a woman giving birth to someone else's child w...  \n",
      "9  some people are highly intelligent but don't d...  \n",
      "  Argument ID  Self-direction: thought  Self-direction: action  Stimulation  \\\n",
      "0      A01001                        0                       0            0   \n",
      "1      A01012                        0                       0            0   \n",
      "2      A02001                        0                       0            0   \n",
      "3      A02002                        0                       1            0   \n",
      "4      A02009                        0                       0            0   \n",
      "5      A02018                        0                       0            0   \n",
      "6      A03005                        0                       0            0   \n",
      "7      A03014                        0                       0            0   \n",
      "8      A03018                        0                       0            0   \n",
      "9      A04005                        0                       0            0   \n",
      "\n",
      "   Hedonism  Achievement  Power: dominance  Power: resources  Face  \\\n",
      "0         0            0                 0                 0     0   \n",
      "1         0            0                 0                 0     0   \n",
      "2         0            0                 0                 0     0   \n",
      "3         0            0                 0                 0     0   \n",
      "4         0            0                 0                 0     0   \n",
      "5         0            0                 0                 0     0   \n",
      "6         0            0                 0                 0     0   \n",
      "7         0            0                 0                 0     0   \n",
      "8         0            0                 0                 0     1   \n",
      "9         0            1                 0                 0     0   \n",
      "\n",
      "   Security: personal  ...  Tradition  Conformity: rules  \\\n",
      "0                   0  ...          0                  0   \n",
      "1                   0  ...          0                  0   \n",
      "2                   1  ...          0                  0   \n",
      "3                   0  ...          0                  0   \n",
      "4                   0  ...          0                  1   \n",
      "5                   0  ...          0                  0   \n",
      "6                   0  ...          0                  1   \n",
      "7                   0  ...          0                  1   \n",
      "8                   1  ...          0                  0   \n",
      "9                   0  ...          0                  0   \n",
      "\n",
      "   Conformity: interpersonal  Humility  Benevolence: caring  \\\n",
      "0                          0         0                    0   \n",
      "1                          0         0                    0   \n",
      "2                          0         0                    0   \n",
      "3                          0         0                    0   \n",
      "4                          0         0                    0   \n",
      "5                          0         0                    0   \n",
      "6                          1         1                    0   \n",
      "7                          0         0                    0   \n",
      "8                          0         0                    0   \n",
      "9                          0         0                    0   \n",
      "\n",
      "   Benevolence: dependability  Universalism: concern  Universalism: nature  \\\n",
      "0                           0                      0                     0   \n",
      "1                           0                      1                     0   \n",
      "2                           0                      1                     0   \n",
      "3                           0                      0                     0   \n",
      "4                           0                      1                     0   \n",
      "5                           0                      0                     0   \n",
      "6                           0                      0                     0   \n",
      "7                           0                      0                     0   \n",
      "8                           0                      0                     0   \n",
      "9                           0                      0                     0   \n",
      "\n",
      "   Universalism: tolerance  Universalism: objectivity  \n",
      "0                        0                          0  \n",
      "1                        0                          0  \n",
      "2                        0                          0  \n",
      "3                        0                          0  \n",
      "4                        0                          1  \n",
      "5                        0                          1  \n",
      "6                        0                          0  \n",
      "7                        0                          0  \n",
      "8                        0                          0  \n",
      "9                        0                          1  \n",
      "\n",
      "[10 rows x 21 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Argument ID</th>\n",
       "      <th>Conclusion</th>\n",
       "      <th>Stance</th>\n",
       "      <th>Premise</th>\n",
       "      <th>Self-direction: action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A01001</td>\n",
       "      <td>Entrapment should be legalized</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>if entrapment can serve to more easily capture...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A01012</td>\n",
       "      <td>The use of public defenders should be mandatory</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>the use of public defenders should be mandator...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A02001</td>\n",
       "      <td>Payday loans should be banned</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>payday loans create a more impoverished societ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A02002</td>\n",
       "      <td>Surrogacy should be banned</td>\n",
       "      <td>against</td>\n",
       "      <td>Surrogacy should not be banned as it is the wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A02009</td>\n",
       "      <td>Entrapment should be legalized</td>\n",
       "      <td>against</td>\n",
       "      <td>entrapment is gravely immoral and</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1888</th>\n",
       "      <td>E08014</td>\n",
       "      <td>We should shift the EU policy toward the Russi...</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>Pushing Russia to the wall will have adverse e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889</th>\n",
       "      <td>E08021</td>\n",
       "      <td>We should stop buying Russian gas</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>The Russians use the money we give them in exc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>E08022</td>\n",
       "      <td>We should stop buying Russian gas</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>The cost of gas will be higher. But I prefer t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>E08024</td>\n",
       "      <td>We should strengthen our ties with Ukraine and...</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>We must support countries that want to improve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>E08025</td>\n",
       "      <td>We should strengthen our ties with Ukraine and...</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>When you see young Ukrainians dying for the EU...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1893 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Argument ID                                         Conclusion  \\\n",
       "0         A01001                     Entrapment should be legalized   \n",
       "1         A01012    The use of public defenders should be mandatory   \n",
       "2         A02001                      Payday loans should be banned   \n",
       "3         A02002                         Surrogacy should be banned   \n",
       "4         A02009                     Entrapment should be legalized   \n",
       "...          ...                                                ...   \n",
       "1888      E08014  We should shift the EU policy toward the Russi...   \n",
       "1889      E08021                  We should stop buying Russian gas   \n",
       "1890      E08022                  We should stop buying Russian gas   \n",
       "1891      E08024  We should strengthen our ties with Ukraine and...   \n",
       "1892      E08025  We should strengthen our ties with Ukraine and...   \n",
       "\n",
       "           Stance                                            Premise  \\\n",
       "0     in favor of  if entrapment can serve to more easily capture...   \n",
       "1     in favor of  the use of public defenders should be mandator...   \n",
       "2     in favor of  payday loans create a more impoverished societ...   \n",
       "3         against  Surrogacy should not be banned as it is the wo...   \n",
       "4         against                  entrapment is gravely immoral and   \n",
       "...           ...                                                ...   \n",
       "1888  in favor of  Pushing Russia to the wall will have adverse e...   \n",
       "1889  in favor of  The Russians use the money we give them in exc...   \n",
       "1890  in favor of  The cost of gas will be higher. But I prefer t...   \n",
       "1891  in favor of  We must support countries that want to improve...   \n",
       "1892  in favor of  When you see young Ukrainians dying for the EU...   \n",
       "\n",
       "      Self-direction: action  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          1  \n",
       "4                          0  \n",
       "...                      ...  \n",
       "1888                       0  \n",
       "1889                       0  \n",
       "1890                       1  \n",
       "1891                       1  \n",
       "1892                       1  \n",
       "\n",
       "[1893 rows x 5 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vali_arguments, df_vali_labels = load_training_data(validation_arg_path, validation_label_path)\n",
    "\n",
    "df_vali_labels = df_vali_labels[[\"Argument ID\", \"Self-direction: action\"]]\n",
    "df_vali_arguments = df_vali_arguments.merge(df_vali_labels, on='Argument ID')\n",
    "df_vali_arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input(dataset: pandas.core.frame.DataFrame) -> (List[str], List[str], List[int]):\n",
    "    \n",
    "    premise, conclusion, label = ([] for i in range(3))\n",
    "    \n",
    "    premise = dataset[\"Premise\"].tolist()\n",
    "    conclusion = (dataset[\"Stance\"] + \": \" + dataset[\"Conclusion\"]).tolist()\n",
    "    label = dataset[\"Self-direction: action\"].tolist()\n",
    "        \n",
    "    return premise, conclusion, label\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "def chunk(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def chunk_multi(lst1, lst2, n):\n",
    "    for i in range(0, len(lst1), n):\n",
    "        yield lst1[i: i + n], lst2[i: i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(labels: List[int]) -> torch.FloatTensor:\n",
    "    \"\"\"Turns the batch of labels into a tensor\n",
    "\n",
    "    Args:\n",
    "        labels (List[int]): List of all labels in the batch\n",
    "\n",
    "    Returns:\n",
    "        torch.FloatTensor: Tensor of all labels in the batch\n",
    "    \"\"\"\n",
    "    return torch.LongTensor([int(l) for l in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2023,  2003,  1996, 18458,  1012,   102,  2023,  2003,  1996,\n",
      "         10744,   102,     0],\n",
      "        [  101,  2023,  2003,  2036,  1037, 18458,   102,  2023,  2003,  1037,\n",
      "          2117, 10744,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS] this is the premise. [SEP] this is the hypothesis [SEP] [PAD]',\n",
       " '[CLS] this is also a premise [SEP] this is a second hypothesis [SEP]']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Huggingface tokenizer\n",
    "\n",
    "class BatchTokenizer:\n",
    "    \"\"\"Tokenizes and pads a batch of input sentences.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the tokenizer\n",
    "\n",
    "        Args:\n",
    "            pad_symbol (Optional[str], optional): The symbol for a pad. Defaults to \"<P>\".\n",
    "        \"\"\"\n",
    "        self.hf_tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-small\")\n",
    "    \n",
    "    def get_sep_token(self,):\n",
    "        return self.hf_tokenizer.sep_token\n",
    "    \n",
    "    def __call__(self, prem_batch: List[str], conc_batch: List[str]) -> List[List[str]]:\n",
    "        \"\"\"Uses the huggingface tokenizer to tokenize and pad a batch.\n",
    "\n",
    "        We return a dictionary of tensors per the huggingface model specification.\n",
    "\n",
    "        Args:\n",
    "            batch (List[str]): A List of sentence strings\n",
    "\n",
    "        Returns:\n",
    "            Dict: The dictionary of token specifications provided by HuggingFace\n",
    "        \"\"\"\n",
    "        # The HF tokenizer will PAD for us, and additionally combine \n",
    "        # The two sentences deimited by the [SEP] token.\n",
    "        enc = self.hf_tokenizer(\n",
    "            prem_batch,\n",
    "            conc_batch,\n",
    "            padding=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return enc\n",
    "    \n",
    "\n",
    "# HERE IS AN EXAMPLE OF HOW TO USE THE BATCH TOKENIZER\n",
    "tokenizer = BatchTokenizer()\n",
    "x = tokenizer(*[[\"this is the premise.\", \"This is also a premise\"], [\"this is the hypothesis\", \"This is a second hypothesis\"]])\n",
    "print(x)\n",
    "tokenizer.hf_tokenizer.batch_decode(x[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model here\n",
    "class HVDClassifier(torch.nn.Module):\n",
    "    def __init__(self, output_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        # Initialize BERT, which we use instead of a single embedding layer.\n",
    "        self.bert = BertModel.from_pretrained(\"prajjwal1/bert-small\")\n",
    "        # TODO [OPTIONAL]: Updating all BERT parameters can be slow and memory intensive. \n",
    "        # Freeze them if training is too slow. Notice that the learning\n",
    "        # rate should probably be smaller in this case.\n",
    "        # Uncommenting out the below 2 lines means only our classification layer will be updated.\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.bert_hidden_dimension = self.bert.config.hidden_size\n",
    "        # TODO: Add an extra hidden layer in the classifier, projecting\n",
    "        #      from the BERT hidden dimension to hidden size.\n",
    "        self.hidden_layer = torch.nn.Linear(self.bert_hidden_dimension, self.hidden_size)\n",
    "        \n",
    "        # TODO: Add a relu nonlinearity to be used in the forward method\n",
    "        #      https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.classifier = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def encode_text(\n",
    "        self,\n",
    "        symbols: Dict\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Encode the (batch of) sequence(s) of token symbols with an LSTM.\n",
    "            Then, get the last (non-padded) hidden state for each symbol and return that.\n",
    "\n",
    "        Args:\n",
    "            symbols (Dict): The Dict of token specifications provided by the HuggingFace tokenizer\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The final hiddens tate of the LSTM, which represents an encoding of\n",
    "                the entire sentence\n",
    "        \"\"\"\n",
    "        # First we get the contextualized embedding for each input symbol\n",
    "        # We no longer need an LSTM, since BERT encodes context and \n",
    "        # gives us a single vector describing the sequence in the form of the [CLS] token.\n",
    "        encoded_sequence = self.bert(**symbols)\n",
    "        # TODO: Get the [CLS] token using the `pooler_output` from \n",
    "        #      The BertModel output. See here: https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel\n",
    "        #      and check the returns for the forward method.\n",
    "        # We want to return a tensor of the form batch_size x 1 x bert_hidden_dimension\n",
    "        output = torch.unsqueeze(encoded_sequence.pooler_output,1)\n",
    "        return output\n",
    "    \n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        symbols: Dict,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            symbols (Dict): The Dict of token specifications provided by the HuggingFace tokenizer\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: _description_\n",
    "        \"\"\"\n",
    "        encoded_sents = self.encode_text(symbols)\n",
    "        output = self.hidden_layer(encoded_sents)\n",
    "        output = self.relu(output)\n",
    "        output = self.classifier(output)\n",
    "        return self.log_softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For making predictions at test time\n",
    "def predict(model: torch.nn.Module, sents: torch.Tensor) -> List:\n",
    "    logits = model(sents)\n",
    "    return list(torch.argmax(logits, axis=2).squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import logical_and, sum as t_sum\n",
    "\n",
    "def precision(predicted_labels, true_labels, which_label=1):\n",
    "    \"\"\"\n",
    "    Precision is True Positives / All Positives Predictions\n",
    "    \"\"\"\n",
    "    pred_which = np.array([pred == which_label for pred in predicted_labels])\n",
    "    true_which = np.array([lab == which_label for lab in true_labels])\n",
    "    denominator = t_sum(pred_which)\n",
    "    if denominator:\n",
    "        return t_sum(logical_and(pred_which, true_which))/denominator\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "\n",
    "def recall(predicted_labels, true_labels, which_label=1):\n",
    "    \"\"\"\n",
    "    Recall is True Positives / All Positive Labels\n",
    "    \"\"\"\n",
    "    pred_which = np.array([pred == which_label for pred in predicted_labels])\n",
    "    true_which = np.array([lab == which_label for lab in true_labels])\n",
    "    denominator = t_sum(true_which)\n",
    "    if denominator:\n",
    "        return t_sum(logical_and(pred_which, true_which))/denominator\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "\n",
    "def f1_score(\n",
    "    predicted_labels: List[int],\n",
    "    true_labels: List[int],\n",
    "    which_label: int\n",
    "):\n",
    "    \"\"\"\n",
    "    F1 score is the harmonic mean of precision and recall\n",
    "    \"\"\"\n",
    "    P = precision(predicted_labels, true_labels, which_label=which_label)\n",
    "    R = recall(predicted_labels, true_labels, which_label=which_label)\n",
    "    if P and R:\n",
    "        return 2*P*R/(P+R)\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "\n",
    "def macro_f1(\n",
    "    predicted_labels: List[int],\n",
    "    true_labels: List[int],\n",
    "    possible_labels: List[int]\n",
    "):\n",
    "    scores = [f1_score(predicted_labels, true_labels, l) for l in possible_labels]\n",
    "    # Macro, so we take the uniform avg.\n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def training_loop(\n",
    "    num_epochs,\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    dev_sents,\n",
    "    dev_labels,\n",
    "    optimizer,\n",
    "    model,\n",
    "):\n",
    "    print(\"Training...\")\n",
    "    loss_func = torch.nn.NLLLoss()\n",
    "    batches = list(zip(train_features, train_labels))\n",
    "    random.shuffle(batches)\n",
    "    for i in range(num_epochs):\n",
    "        losses = []\n",
    "        for features, labels in tqdm(batches):\n",
    "            # Empty the dynamic computation graph\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(features).squeeze(1)\n",
    "            loss = loss_func(preds, labels)\n",
    "            # Backpropogate the loss through our model\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        print(f\"epoch {i}, loss: {sum(losses)/len(losses)}\")\n",
    "        # Estimate the f1 score for the development set\n",
    "        print(\"Evaluating dev...\")\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        for sents, labels in tqdm(zip(dev_sents, dev_labels), total=len(dev_sents)):\n",
    "            pred = predict(model, sents)\n",
    "            all_preds.extend(pred)\n",
    "            all_labels.extend(list(labels.numpy()))\n",
    "\n",
    "        dev_f1 = macro_f1(all_preds, all_labels, [0,1])\n",
    "        print(f\"Dev F1 {dev_f1}\")\n",
    "        \n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch and Tokenize data here\n",
    "tokenizer = BatchTokenizer()\n",
    "\n",
    "# Traning dataset\n",
    "train_premises, train_conclusions, train_labels = generate_input(df_train_arguments)\n",
    "# Batches\n",
    "train_input_batches = [b for b in chunk_multi(train_premises, train_conclusions, batch_size)]\n",
    "train_label_batches = [b for b in chunk(train_labels, batch_size)]\n",
    "# Tokenize + encode\n",
    "train_input_batches = [tokenizer(*batch) for batch in train_input_batches]\n",
    "train_label_batches = [encode_labels(batch) for batch in train_label_batches]\n",
    "\n",
    "# Validation dataset\n",
    "vali_premises, vali_conclusions, vali_labels = generate_input(df_vali_arguments)\n",
    "# Batches\n",
    "vali_input_batches = [b for b in chunk_multi(vali_premises, vali_conclusions, batch_size)]\n",
    "vali_label_batches = [b for b in chunk(vali_labels, batch_size)]\n",
    "# Tokenize + encode\n",
    "vali_input_batches = [tokenizer(*batch) for batch in vali_input_batches]\n",
    "vali_label_batches = [encode_labels(batch) for batch in vali_label_batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-small were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 653/653 [00:36<00:00, 18.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 0.5491673522621713\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 237/237 [00:15<00:00, 15.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.42694014119031276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 653/653 [00:33<00:00, 19.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 0.5154772237468095\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 237/237 [00:14<00:00, 16.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.4671328817443076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 653/653 [00:33<00:00, 19.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss: 0.5018063504676724\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 237/237 [00:15<00:00, 15.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.5087990288883444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 653/653 [00:33<00:00, 19.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, loss: 0.4933519338301824\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 237/237 [00:14<00:00, 16.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.5191160973147378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 653/653 [00:33<00:00, 19.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, loss: 0.487600623853904\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 237/237 [00:14<00:00, 16.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.5313513469970813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 653/653 [00:33<00:00, 19.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, loss: 0.4830247539039958\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 237/237 [00:14<00:00, 16.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.5491069471636152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 653/653 [00:33<00:00, 19.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, loss: 0.47935281344796393\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 237/237 [00:14<00:00, 16.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.5603201506591337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 653/653 [00:34<00:00, 19.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, loss: 0.47647733547101523\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 237/237 [00:14<00:00, 16.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.5688545635611236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 653/653 [00:33<00:00, 19.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, loss: 0.4738991100563572\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 237/237 [00:14<00:00, 16.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.576917903535721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 653/653 [00:34<00:00, 19.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, loss: 0.4715468457017157\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 237/237 [00:14<00:00, 15.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.5836161387631976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 653/653 [00:33<00:00, 19.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, loss: 0.4694790890009626\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 237/237 [00:14<00:00, 15.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.5899935022742041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 653/653 [00:33<00:00, 19.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11, loss: 0.4678639250067255\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 237/237 [00:14<00:00, 16.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.5935420562998162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 653/653 [00:33<00:00, 19.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12, loss: 0.4664178371748917\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 237/237 [00:14<00:00, 16.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.5951484436264185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 653/653 [00:34<00:00, 19.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13, loss: 0.465084907112452\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 237/237 [00:15<00:00, 15.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.5971631170493242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 653/653 [00:33<00:00, 19.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14, loss: 0.46390028524385113\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 237/237 [00:15<00:00, 15.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.6004762074623671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 653/653 [00:34<00:00, 18.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15, loss: 0.46270275421178175\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 237/237 [00:14<00:00, 16.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.6029737027073357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 653/653 [00:33<00:00, 19.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16, loss: 0.46155814851019505\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 237/237 [00:14<00:00, 15.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.6062441130298273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 653/653 [00:34<00:00, 19.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17, loss: 0.46048997788708573\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 237/237 [00:14<00:00, 15.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.6090542672685487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 653/653 [00:41<00:00, 15.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18, loss: 0.45968303405558536\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 237/237 [00:15<00:00, 15.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.6076510925407361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 653/653 [00:29<00:00, 22.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19, loss: 0.458705662648446\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 237/237 [00:15<00:00, 15.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.6076510925407361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 653/653 [00:36<00:00, 17.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20, loss: 0.45792025317413704\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 237/237 [00:16<00:00, 14.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.6104536551817852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 653/653 [00:29<00:00, 21.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21, loss: 0.45718367715547314\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 237/237 [00:12<00:00, 18.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.6113950375584221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 653/653 [00:34<00:00, 18.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22, loss: 0.4564702615917733\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 237/237 [00:15<00:00, 14.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.6164747280870121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 653/653 [00:36<00:00, 18.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23, loss: 0.4557794091815149\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 237/237 [00:16<00:00, 14.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.6178488350983359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 653/653 [00:37<00:00, 17.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24, loss: 0.45513034261660407\n",
      "Evaluating dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 237/237 [00:16<00:00, 14.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 0.6233089934062969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HVDClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 512, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 512)\n",
       "      (token_type_embeddings): Embedding(2, 512)\n",
       "      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (hidden_layer): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (classifier): Linear(in_features=2, out_features=2, bias=True)\n",
       "  (log_softmax): LogSoftmax(dim=2)\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can increase epochs if need be\n",
    "epochs = 25\n",
    "# TODO: Find a good learning rate\n",
    "LR = 0.001\n",
    "\n",
    "possible_labels = len(set(train_labels))\n",
    "model = HVDClassifier(output_size=possible_labels, hidden_size=2)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), LR)\n",
    "\n",
    "training_loop(\n",
    "    epochs,\n",
    "    train_input_batches,\n",
    "    train_label_batches,\n",
    "    vali_input_batches,\n",
    "    vali_label_batches,\n",
    "    optimizer,\n",
    "    model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
